# LSTM
- 過去の情報をネットワーク内に保尊したい
- 過去の情報を必要なときのみに取得、書き換えをしたい

## BPTT
時系列モデルではその時間の出力層だけでなく未来の隠れ層からも誤差を受け取る

## 概念
- CEC：誤差を消失させず、誤差をとどまらせる機能がある。
- ゲート：学習がいったりきたりするのを防ぐ。入力ゲートと出力ゲート
- 忘却ゲート：CECの伝搬する値に0~1で制御する
- のぞき穴結合：各ゲートがCECを用いることができるようにしたもの。誤差逆伝播時に出力が閉じてると他のゲートも学習できないため？

## 気をつけること
- 直交行列を用いた重み初期化(np.linalg.svd()で求められる)
- ReLUを用いると発散する。シグモイドかtanh
- 勾配消失と同様にLSTMも時間をまたぎ続けると勾配がなくなりやすい

## モデルの構築
### keras
- 全結合の中間層を持つRNN```SimpleRNN(ユニット数, activation=活性化関数, ...)```

### tf


### pytorch


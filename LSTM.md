# LSTM
- 過去の情報をネットワーク内に保尊したい
- 過去の情報を必要なときのみに取得、書き換えをしたい

## BPTT
時系列モデルではその時間の出力層だけでなく未来の隠れ層からも誤差を受け取る

## 概念
- CEC：誤差を消失させず、誤差をとどまらせる機能がある。
- ゲート：学習がいったりきたりするのを防ぐ。入力ゲートと出力ゲート
- 忘却ゲート：CECの伝搬する値に0~1で制御する
- のぞき穴結合：各ゲートがCECを用いることができるようにしたもの。誤差逆伝播時に出力が閉じてると他のゲートも学習できないため？
- GRU：LSTMより学習がかなり早い。リセットゲートと更新ゲートのみで構成
- 双方向RNN：例えば文章の場合はあとの単語から前の単語の形態素がわかったりする。
- NNはワンホットしか受け取らない。埋め込み層はID列からワンホットの列に変換する処理を行っている。
- 対訳コーパス：英語と日本語の例文のセット
- シーケンス：時系列データのこと

# Encoder-Decoder
出力を時系列的にするのに用いる。
- エンコーダ―：入力を処理するもの。
- デコーダ―：出力を処理するもの。
- 対訳コーパス：翻訳のために用意された言語対の文例
- トークン：形態素などの基準のこと
- 辞書：各トークンにIDを振ったもの。エンコーダーでは単語からID列へ、デコーダーではID列から単語へ変換する。
- イテレーター：繰り返し処理に適していて順番にデータを取り出せるもの
- 教師強制：デコーダーの次の入力を出力ではなく教師ベクトルによって行う。
デコーダ―はエンコーダーの時系列情報を引き継がないと学習ができない。最後の入力の後の隠れ層の値をデコーダーの初期値に設定する(文脈ベクトル)。  
ミニバッチ事の長さでパディングを行う。

# Attention
Encoderでは何も行わず、ただただ加えられた入力データを一つのベクトルとしてデコーダーに伝えているだけである。  
Attentionではでコーターの各時刻において、入力シーケンスの各時刻における値を加重平均したものを用いる。
- スコア関数：加重平均の際の各入力の重みを決める関数


## 気をつけること
- 直交行列を用いた重み初期化(np.linalg.svd()で求められる)
- ReLUを用いると発散する。シグモイドかtanh
- 勾配消失と同様にLSTMも時間をまたぎ続けると勾配がなくなりやすい

## 実際に使うとき
- 直前までの一定個のデータで次を予測
- 予測した次とそれまでのデータを用いて次を予測
直前のデータをいくつ使うか、どこまで予測させるかで使い分けること

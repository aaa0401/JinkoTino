# LSTM
- 過去の情報をネットワーク内に保尊したい
- 過去の情報を必要なときのみに取得、書き換えをしたい

## BPTT
時系列モデルではその時間の出力層だけでなく未来の隠れ層からも誤差を受け取る

## 概念
- CEC：誤差を消失させず、誤差をとどまらせる機能がある。
- ゲート：学習がいったりきたりするのを防ぐ。入力ゲートと出力ゲート
- 忘却ゲート：CECの伝搬する値に0~1で制御する
- のぞき穴結合：各ゲートがCECを用いることができるようにしたもの。誤差逆伝播時に出力が閉じてると他のゲートも学習できないため？
- GRU：LSTMより学習がかなり早い。リセットゲートと更新ゲートのみで構成
- 双方向RNN：例えば文章の場合はあとの単語から前の単語の形態素がわかったりする。
- NNはワンホットしか受け取らない。埋め込み層はID列からワンホットの列に変換する処理を行っている。
- 対訳コーパス：英語と日本語の例文のセット
- シーケンス：時系列データのこと
- Seq2Seq：

## 気をつけること
- 直交行列を用いた重み初期化(np.linalg.svd()で求められる)
- ReLUを用いると発散する。シグモイドかtanh
- 勾配消失と同様にLSTMも時間をまたぎ続けると勾配がなくなりやすい

## モデルの構築

## 実際に使うとき
- 直前までの一定個のデータで次を予測
- 予測した次とそれまでのデータを用いて次を予測
直前のデータをいくつ使うか、どこまで予測させるかで使い分けること
